{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 376 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory='../../Data/motion detector/train',\n",
    "                                                   target_size = (64,64),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   batch_size = 16,\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   shuffle = True,\n",
    "                                                   seed = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = ImageDataGenerator( rescale = 1./255)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(directory='../../Data/motion detector/validation',\n",
    "                                                   target_size = (64,64),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   batch_size = 16,\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   shuffle = True,\n",
    "                                                   seed = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(64,64,3)))\n",
    "    model.add(tf.keras.layers.Conv2D(6, (3,3), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(128))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(64))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(3))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 13s 548ms/step - loss: 0.9665 - categorical_accuracy: 0.4840 - val_loss: 0.5375 - val_categorical_accuracy: 0.8452\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 7s 292ms/step - loss: 0.5608 - categorical_accuracy: 0.7872 - val_loss: 0.2473 - val_categorical_accuracy: 0.8690\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 6s 268ms/step - loss: 0.4240 - categorical_accuracy: 0.8324 - val_loss: 0.2516 - val_categorical_accuracy: 0.9286\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 7s 278ms/step - loss: 0.3914 - categorical_accuracy: 0.8644 - val_loss: 0.2319 - val_categorical_accuracy: 0.9405\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 7s 273ms/step - loss: 0.3423 - categorical_accuracy: 0.8777 - val_loss: 0.2251 - val_categorical_accuracy: 0.9405\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 7s 274ms/step - loss: 0.3456 - categorical_accuracy: 0.8750 - val_loss: 0.2033 - val_categorical_accuracy: 0.9405\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 7s 281ms/step - loss: 0.2983 - categorical_accuracy: 0.8936 - val_loss: 0.2147 - val_categorical_accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 6s 269ms/step - loss: 0.3169 - categorical_accuracy: 0.9122 - val_loss: 0.7386 - val_categorical_accuracy: 0.8214\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 7s 287ms/step - loss: 0.3977 - categorical_accuracy: 0.8564 - val_loss: 0.2418 - val_categorical_accuracy: 0.9286\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.2947 - categorical_accuracy: 0.9016 - val_loss: 0.2600 - val_categorical_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73cc3eb8d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(train_generator,\n",
    "         epochs=10,\n",
    "         validation_data=validation_generator,\n",
    "         validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Generated\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('../../Data/motion detector/test_video.mov')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "out = cv2.VideoWriter('output_count.avi',fourcc,20.0,(int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "ret, frame1 =cap.read()\n",
    "# cvtColor() method is used to convert an image from one color space to another\n",
    "# we use the function cv2. cvtColor(input_image, flag) where flag determines the type of conversion. For BGR Gray conversion we use the flags cv2.COLOR_BGR2GRAY\n",
    "\n",
    "prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "# Create mask\n",
    "hsv = np.zeros_like(frame1)\n",
    "# Set image saturation to maximum value as we do not need it\n",
    "hsv[...,1] = 255\n",
    "\n",
    "i=0\n",
    "prediction_str = \"\"\n",
    "repetitions=0\n",
    "up=0\n",
    "down=0\n",
    "no_move=0\n",
    "current_move=0\n",
    "initial =-1\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    i+=1\n",
    "    # Read a frame from video\n",
    "    ret, frame2 = cap.read()\n",
    "    if not(ret):\n",
    "        break\n",
    "    # Convert new frame format`s to gray scale\n",
    "    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate dense optical flow by Farneback method\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute the magnitude and angle of the 2D vectors\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0],flow[...,1])\n",
    "    # Set image hue according to the optical flow direction\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    # Set image value according to the optical flow magnitude (normalized)\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    # Convert HSV to RGB (BGR) color representation\n",
    "    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    # Resize frame size to match dimensions\n",
    "    image = cv2.resize(rgb, (64, 64))\n",
    "    image = image.reshape((1,) + image.shape)\n",
    "    image = image/255.0\n",
    "    prediction = np.argmax(model.predict(image), axis=-1)[0]\n",
    "    \n",
    "    if prediction == 0:\n",
    "        down +=1 \n",
    "        if down == 3:\n",
    "          if initial == -1:\n",
    "            initial = 0\n",
    "          if current_move == 2:\n",
    "            repetitions+=1\n",
    "          current_move = 0\n",
    "        elif down > 0:\n",
    "          up = 0\n",
    "          no_move = 0\n",
    "    elif prediction == 2:\n",
    "        up += 1\n",
    "        if up == 3 and initial != -1:\n",
    "          current_move = 2\n",
    "        elif up > 1:\n",
    "          down = 0 \n",
    "          no_move = 0\n",
    "    else:\n",
    "        no_move += 1\n",
    "        if no_move == 15:\n",
    "          current_move = 1\n",
    "        elif no_move > 10:\n",
    "          up = 0\n",
    "          down = 0 \n",
    "    font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (10,400)\n",
    "    fontScale              = 1\n",
    "    fontColor              = (255,255,255)\n",
    "    lineType               = 5\n",
    "    cv2.putText(frame2, \"Repetitions: \"+ str(repetitions),bottomLeftCornerOfText,font, fontScale,fontColor,lineType)\n",
    "    out.write(frame2)\n",
    "    prvs = next\n",
    "\n",
    "print(\"Video Generated\")\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
